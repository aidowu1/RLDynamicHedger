{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68b3f8f-410d-4c42-a452-c5b20b1bcbd3",
   "metadata": {},
   "source": [
    "### RLDynamicHedger Hyper-parameter Tuning For All RL Algorithms\n",
    " - This notebook is used to demo the hyper-parameter tuning of DDPG, TD3, SAC and PPO RL algorithms\n",
    " - It also persists the best model trained with the searched/optimal hyper-parameters\n",
    " - Currently setup for RL algorithm: DDPG and simulation use case: GBM (note that you can changes the RL algorithm and hedging simulation settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3484650c-fef2-49ea-adb0-bd46ecab00da",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30f4d37-e5ed-4cd0-a4b7-2239cc8f77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "SEED = 100\n",
    "NEW_LINE = \"\\n\"\n",
    "LINE_DIVIDER = \"==========\" * 5\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c5045-574c-4d02-b0f1-52cd12870455",
   "metadata": {},
   "source": [
    "#### Import the experiment use cases module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351ac8dd-6710-4933-9ea7-0360b3bf76ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path is: C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project\\RLDynamicHedgerV2...\n",
      "\n",
      "Root folder: C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project\\RLDynamicHedgerV2\\scripts\\..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADE\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from experiment_use_cases import run_scenario_map, getRunScenarioParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d04cc-d0f9-4306-b99b-cf2ad06d3876",
   "metadata": {},
   "source": [
    "#### Set current working directory.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f89c0cc-2c64-4dba-bae7-82b6bef8a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path is: C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = \"../\"\n",
    "os.chdir(ROOT_PATH)\n",
    "sys.path.insert(1, ROOT_PATH)\n",
    "print(f\"Current path is: {os.getcwd()}...{NEW_LINE}\")\n",
    "\n",
    "#### Libaries for RLDynamicHedger\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c031ab-9ab0-4596-acbe-062cc62b934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.utility.enum_types import PlotType, AggregationType, HedgingType, RLAgorithmType\n",
    "from src.main.market_simulator.parameters import Parameters\n",
    "from src.main.utility.utils import Helpers\n",
    "from scripts.tune_hedger_rl_model import TuneHyperparametersForRLModels\n",
    "import src.main.configs as configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938cddc-1818-47be-9dbe-07347dedb52b",
   "metadata": {},
   "source": [
    "#### Set demo use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14bc9636-b1b5-4c40-b23d-8a8c0f0c84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEDGING_TYPE = HedgingType.gbm\n",
    "RL_ALGO_TYPE = RLAgorithmType.ppo\n",
    "MODEL_USE_CASE = \"low_moneyness\"\n",
    "# USE_CASES = [\"low_expiry\", \"high_expiry\", \"low_trading_cost\", \"high_trading_cost\", \"low_trading_freq\",\"high_trading_freq\", \"high_moneyness\", \"low_moneyness\"]\n",
    "USE_CASES = [\"low_moneyness\"]\n",
    "# ALGO_TYPES = [RLAgorithmType.ppo, RLAgorithmType.ddpg, RLAgorithmType.td3, RLAgorithmType.sac]\n",
    "ALGO_TYPES = [RLAgorithmType.ddpg]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e203af-65fc-4851-9381-776a71be41fd",
   "metadata": {},
   "source": [
    "#### Run the hyper-parameter tuning/training cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e745bc30-54fe-4b8e-90bf-4bb701ded2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTuningCyles(\n",
    "    model_use_case: str = MODEL_USE_CASE,\n",
    "    algo_type: RLAgorithmType = RL_ALGO_TYPE,\n",
    "    hedging_type: HedgingType = HEDGING_TYPE\n",
    "):\n",
    "    \"\"\"\n",
    "    Entry point to run the RL hyper-parameter tuning/training cycles\n",
    "    :param model_use_case: Model use case\n",
    "    :param algo_type: Algorithm type\n",
    "    :param hedging_type: Hedging type    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    start_time = time.process_time()\n",
    "    print(f\"Start of RL agent hyper-parameter tuning/training cycles for RL agorithm: {algo_type} and simulation hedging use case: {hedging_type}\\n\")\n",
    "    parameter_settings_data = Helpers.getParameterSettings(configs.DEFAULT_SETTINGS_NAME)\n",
    "    is_test_env = False\n",
    "    parameters = Parameters(**parameter_settings_data)\n",
    "    \n",
    "    run_scenario_parameters = getRunScenarioParams(parameters, scenario=model_use_case, is_test_env=is_test_env)\n",
    "    model = TuneHyperparametersForRLModels(algo_type, hedging_type, run_scenario_parameters, model_use_case=model_use_case)\n",
    "    \n",
    "    best_model_path = model.hyperparameterTuningCyle()\n",
    "    end_time = time.process_time()\n",
    "    elapsed_time_sec = round(end_time - start_time, 4)\n",
    "    elapsed_time_min = round(elapsed_time_sec/60, 4)\n",
    "    print(f\"End of tunning cycles, the best hyper-parameter model is saved at this folder: {best_model_path}\")\n",
    "    print(f\"Processing time was: {elapsed_time_sec} seconds | {elapsed_time_min} minutes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38519173-fd81-4753-b192-020115a19397",
   "metadata": {},
   "source": [
    "#### Run the RL tuning/training cycles for all the use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4769f7e9-9f88-4249-8849-7693ed7cc073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:52:29,940 - INFO - tune_hedger_rl_model.py:__init__ - : RL Delta Hedger for ddpg algorithm type in C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project\\RLDynamicHedgerV2\\scripts\\tune_hedger_rl_model.py:50\n",
      "2025-04-20 16:52:29,947 - INFO - env_v2.py:__init__ - : parameters:\n",
      "Parameters(n_paths=1000, n_time_steps=252, n_days_per_year=252, trading_frequency=1, option_expiry_time=1.0, start_stock_price=80.0, strike_price=100, volatility=0.2, start_volatility=0.2, volatility_of_volatility=0.6, risk_free_rate=0.0, dividend_rate=0.0, return_on_stock=0.05, cost_per_traded_stock=0.01, rho=-0.4, stdev_coefficient=1.5, central_difference_spacing=0.01, notional=100, is_reset_path=False, is_test_env=False, hedging_type=<HedgingType.gbm: 1>, maturity_in_months=12, n_business_days=20, volatility_mean_reversion=1.0, long_term_volatility=0.04, volatility_correlation=-0.7, hedging_time_step=0.003968253968253968, trading_cost_parameter=0.003, risk_averse_level=0.1, is_include_option_price_feature=True, epsilon=1.0, tick_size=0.01, is_in_the_money='OTM', is_high_expiry_level=False, frequency_level='low', evaluation_path_index=0, heston_vol_of_vol=0.25, heston_start_vol=0.2)\n",
      " in C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project\\RLDynamicHedgerV2\\src\\main\\environment\\env_v2.py:37\n",
      "2025-04-20 16:52:29,950 - INFO - caching.py:asset_price_data - : Getting asset price data from data/12month/1d/OTM/asset_price_gbm_simulation.csv in C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project\\RLDynamicHedgerV2\\src\\main\\market_simulator\\caching.py:61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of RL agent hyper-parameter tuning/training cycles for RL agorithm: RLAgorithmType.ddpg and simulation hedging use case: HedgingType.gbm\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:52:30,100 - INFO - caching.py:option_price_data - : Getting option price data from data/12month/1d/OTM/option_price_gbm_simulation.csv in C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project\\RLDynamicHedgerV2\\src\\main\\market_simulator\\caching.py:88\n",
      "2025-04-20 16:52:30,209 - INFO - caching.py:option_delta_data - : Getting option delta data from data/12month/1d/OTM/option_delta_gbm_simulation.csv in C:\\Development\\Training\\MLI Certificate of Finance\\Final-Project\\Project\\RLDynamicHedgerV2\\src\\main\\market_simulator\\caching.py:114\n",
      "C:\\Users\\ADE\\miniconda3\\envs\\hedging_env_3\\lib\\site-packages\\gymnasium\\spaces\\box.py:305: UserWarning: \u001B[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001B[0m\n",
      "  gym.logger.warn(\n",
      "[I 2025-04-20 16:52:30,350] A new study created in memory with name: no-name-e6ad07b3-657c-4a1a-b36d-1cca5c6002b1\n",
      "[I 2025-04-20 17:00:33,787] Trial 0 finished with value: -2.50748 and parameters: {'gamma': 0.9999, 'learning_rate': 0.09129660360504312, 'batch_size': 1024, 'buffer_size': 1000000, 'tau': 0.001, 'train_freq': 256, 'noise_type': 'normal', 'noise_std': 0.6636636392431936, 'net_arch': 'big', 'activation_fn': 'relu'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -2.50748 +/-0.0 for 6300 steps\n",
      "New best model saved with mean_reward: -2.51\n",
      "Number of reward values = 134410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:03:19,266] Trial 1 finished with value: -2.5074879999999995 and parameters: {'gamma': 0.9, 'learning_rate': 6.623556125014992e-05, 'batch_size': 32, 'buffer_size': 1000000, 'tau': 0.08, 'train_freq': 64, 'noise_type': None, 'noise_std': 0.9376361871441395, 'net_arch': 'big', 'activation_fn': 'relu'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -2.5074879999999995 +/-4.440892098500626e-16 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:05:42,438] Trial 2 finished with value: -27.052115000000004 and parameters: {'gamma': 0.995, 'learning_rate': 2.623559867509604e-05, 'batch_size': 512, 'buffer_size': 100000, 'tau': 0.02, 'train_freq': 128, 'noise_type': None, 'noise_std': 0.5392938225513774, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -27.052993000000004 +/-3.552713678800501e-15 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:08:44,329] Trial 3 finished with value: -2.5074870000000002 and parameters: {'gamma': 0.9, 'learning_rate': 0.0005604719625679205, 'batch_size': 100, 'buffer_size': 100000, 'tau': 0.01, 'train_freq': 8, 'noise_type': None, 'noise_std': 0.23358901359925022, 'net_arch': 'big', 'activation_fn': 'elu'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -2.5074870000000002 +/-4.440892098500626e-16 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:10:54,805] Trial 4 finished with value: -2.5105180000000002 and parameters: {'gamma': 0.95, 'learning_rate': 0.0002477532689757198, 'batch_size': 128, 'buffer_size': 10000, 'tau': 0.05, 'train_freq': 128, 'noise_type': 'ornstein-uhlenbeck', 'noise_std': 0.6239015357809244, 'net_arch': 'small', 'activation_fn': 'elu'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -2.510106 +/-0.0 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:11:22,638] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -27.064134000000003 +/-3.552713678800501e-15 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:14:23,851] Trial 6 finished with value: -2.50748 and parameters: {'gamma': 0.9999, 'learning_rate': 0.0623668295222346, 'batch_size': 64, 'buffer_size': 1000000, 'tau': 0.001, 'train_freq': 256, 'noise_type': 'normal', 'noise_std': 0.05240783717258801, 'net_arch': 'big', 'activation_fn': 'relu'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -2.50748 +/-0.0 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:16:51,312] Trial 7 finished with value: -2.50748 and parameters: {'gamma': 0.999, 'learning_rate': 0.015210125506577413, 'batch_size': 16, 'buffer_size': 10000, 'tau': 0.005, 'train_freq': 4, 'noise_type': 'normal', 'noise_std': 0.7294958607852513, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -2.50748 +/-0.0 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:16:57,921] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -9.987779 +/-0.0 for 6300 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 17:20:47,920] Trial 9 finished with value: -2.50748 and parameters: {'gamma': 0.98, 'learning_rate': 0.005789174625490958, 'batch_size': 256, 'buffer_size': 1000000, 'tau': 0.05, 'train_freq': 1, 'noise_type': 'normal', 'noise_std': 0.7491674414056981, 'net_arch': 'big', 'activation_fn': 'leaky_relu'}. Best is trial 0 with value: -2.50748.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reward: -2.50748 +/-0.0 for 6300 steps\n",
      "Number of finished trials:  10\n",
      "Best trial:\n",
      "  Value:  -2.50748\n",
      "  Params: \n",
      "    gamma: 0.9999\n",
      "    learning_rate: 0.09129660360504312\n",
      "    batch_size: 1024\n",
      "    buffer_size: 1000000\n",
      "    tau: 0.001\n",
      "    train_freq: 256\n",
      "    noise_type: normal\n",
      "    noise_std: 0.6636636392431936\n",
      "    net_arch: big\n",
      "    activation_fn: relu\n",
      "  User attrs:\n",
      "Hyper-parameter tuning results will be written to this file: tuning_results.csv\n",
      "Plot results of the optimization can be found here: model/trained-tuned-models/ddpg/low_moneyness/_tuning_optimization_history.html and model/trained-tuned-models/ddpg/low_moneyness/tuning_param_importance.html\n",
      "The best hyper-parameters computed have been written to model/trained-tuned-models/ddpg/low_moneyness/tuning_best_values.pkl\n",
      "End of tunning cycles, the best hyper-parameter model is saved at this folder: model/trained-tuned-models/ddpg/low_moneyness/best_model\n",
      "Processing time was: 1684.3281 seconds | 28.0721 minutes\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    \"\"\"\n",
    "    Run the RL tuning/training cycles\n",
    "    \"\"\"\n",
    "    use_cases = USE_CASES\n",
    "    algo_types = ALGO_TYPES    \n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        for algo_type in algo_types:\n",
    "           runTuningCyles(\n",
    "               model_use_case=use_case,\n",
    "               algo_type=algo_type,\n",
    "               hedging_type=HedgingType.gbm\n",
    "           )\n",
    "           print(f\"{LINE_DIVIDER}\\n\\n\\n\")\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c8f68-6904-4570-b896-2ad65e0c825c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e2204-f746-4d08-acfb-0c651d2cad07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d7a9b-77d6-4f77-ae27-1800df4277a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
